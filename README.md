# Project Title: GPT1-LoRA:
  
*  Designed and developed a decoder-based Generative Pretrained Transformer (GPT) model from scratch, achieving a 78% reduction in training loss (from 4.37 to 0.94) through the implementation of multi-headed attention, positional encoding, and dropout with layer normalization.
  
*  Demonstrated good performance in natural language understanding and generation, with a 79% reduction in inference loss (from 5.21 to 1.08) on unseen data, showcasing improved model generalizability and robustness.

*  Using LoRA finetuned GPT1-M model to get 75% accuracy on the CoLA dataset
